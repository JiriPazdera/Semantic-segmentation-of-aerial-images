{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T18:00:04.574105800Z",
     "start_time": "2024-03-27T18:00:04.567828200Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from patchify import patchify\n",
    "from enum import Enum\n",
    "import segmentation_models as sm\n",
    "from tensorflow.keras.metrics import MeanIoU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from simple_multi_unet_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc87296d5936c996",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T17:59:38.579769300Z",
     "start_time": "2024-03-27T17:59:38.568593200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = [0.1666, 0.1666, 0.1666, 0.1666, 0.1666, 0.1666]\n",
    "dice_loss = sm.losses.DiceLoss(class_weights=weights)\n",
    "focal_loss = sm.losses.CategoricalFocalLoss()\n",
    "total_loss = dice_loss + (1 * focal_loss)\n",
    "\n",
    "def jacard_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6efd1e7280f91d7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-27T17:59:38.945556600Z",
     "start_time": "2024-03-27T17:59:38.579769300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_model = r\"C:\\Users\\jiric\\Documents\\VUT\\BP\\DATASETS\\CUSTOM_CZ_DATASET\\MODELS\\model_512_40_DSV30LITE_fd.h5\"\n",
    "save_path = r\"C:\\Users\\jiric\\Documents\\VUT\\BP\\OBRAZKY\\General\\A star\\PATH_C\\UBRO93_pred2.png\"\n",
    "# image_path = r\"C:\\Users\\jiric\\Documents\\VUT\\BP\\DATASETS\\Custom_CZ_sample_work\\Original Tiles\\VYSKOV12\\WRTO24.2022.VYSK12.jpg\"\n",
    "# save_path = r\"C:\\Users\\jiric\\Documents\\VUT\\BP\\OBRAZKY\\BIG_FORMAT_RESULTS\\CUSTOM\\TRAIN RESULTS\\VY12_512_40_DSV30LITE_fd.png\"\n",
    "image_path = r\"C:\\Users\\jiric\\Documents\\VUT\\BP\\OBRAZKY\\General\\A star\\PATH_C\\UBRO93_img2.jpg\"\n",
    "save_bool = 1\n",
    "\n",
    "model = tensorflow.keras.models.load_model(path_model, custom_objects={'jacard_coef':jacard_coef, 'dice_loss_plus_1focal_loss':total_loss})\n",
    "\n",
    "patch_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a1e3dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = {\n",
    "    0: [50, 50, 50],        # Unlabelled\n",
    "    1: [50, 255, 167],      # Vegetation\n",
    "    2: [255, 140, 0],     # Unpaved road\n",
    "    3: [109, 50, 255],      # Paved area\n",
    "    4: [255, 225, 50],      # Building\n",
    "    5: [50, 167, 255],       # Water\n",
    "    6: [166, 117, 86]         # Land\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8950a8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10.0\n"
     ]
    }
   ],
   "source": [
    "matrix = np.array((10, 10, 10))\n",
    "print(matrix[0])\n",
    "matrix_f = matrix.astype(np.float32)\n",
    "print(matrix_f[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6630ade360e98189",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T15:11:13.723667800Z",
     "start_time": "2024-03-28T15:11:12.443915Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original image: (2560, 2560, 3)\n",
      "cropped image: (2560, 2560, 3)\n",
      "initial patches: (5, 5, 1, 512, 512, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nplt.imshow(image_RGB)\\nplt.show()\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image_path = \"C:/Users/jiric/Documents/VUT/BP/DATASETS/CUSTOM_CZ_DATASET/Semantic segmentation dataset/Tile 8/images/VYSK111_0.jpg\"\n",
    "# image_path = r\"C:\\Users\\jiric\\Documents\\VUT\\BP\\OBRAZKY\\BIG_FORMAT_RESULTS\\CUSTOM\\WRTO24.2022.OLOM88.jpg\"\n",
    "# image_path = r\"C:\\Users\\jiric\\Documents\\VUT\\BP\\OBRAZKY\\BIG_FORMAT_RESULTS\\CUSTOM\\WRTO24.2022.VYSK11.jpg\"\n",
    "\n",
    "\n",
    "image_BGR = cv2.imread(image_path)\n",
    "print(\"original image:\", image_BGR.shape)\n",
    "image_RGB = cv2.cvtColor(image_BGR, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "height = image_RGB.shape[0]\n",
    "width = image_RGB.shape[1]\n",
    "\n",
    "new_h = height - (height % patch_size)\n",
    "new_w = width - (width % patch_size)\n",
    "\n",
    "image_RGB = (image_RGB[: new_h, :new_w, :])/255\n",
    "\n",
    "image_RGB = image_RGB.astype(np.float32)\n",
    "\n",
    "print(\"cropped image:\", image_RGB.shape)\n",
    "\n",
    "patches = patchify(image_RGB,(patch_size, patch_size, 3), step=patch_size)\n",
    "print(\"initial patches:\", patches.shape)\n",
    "\n",
    "pred_image = np.zeros_like(image_RGB)\n",
    "\n",
    "\"\"\"\n",
    "plt.imshow(image_RGB)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fdb61a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 1\n",
      "0 2\n",
      "0 3\n",
      "0 4\n",
      "1 0\n",
      "1 1\n",
      "1 2\n",
      "1 3\n",
      "1 4\n",
      "2 0\n",
      "2 1\n",
      "2 2\n",
      "2 3\n",
      "2 4\n",
      "3 0\n",
      "3 1\n",
      "3 2\n",
      "3 3\n",
      "3 4\n",
      "4 0\n",
      "4 1\n",
      "4 2\n",
      "4 3\n",
      "4 4\n",
      "(2560, 2560, 3)\n"
     ]
    }
   ],
   "source": [
    "for i in range(patches.shape[0]):\n",
    "    for j in range(patches.shape[1]):\n",
    "\n",
    "        print(i,j)\n",
    "\n",
    "        patch = patches[i,j]\n",
    "\n",
    "        predicted_patch = model.predict(patch)\n",
    "        \n",
    "\n",
    "        predicted_patch = predicted_patch[0,:,:,:]\n",
    "\n",
    "        class_matrix = np.argmax(predicted_patch, axis=-1)\n",
    "\n",
    "        predicted_patch_rgb = np.zeros((patch_size, patch_size,3))\n",
    "\n",
    "        for class_id, color in colormap.items():\n",
    "            predicted_patch_rgb[class_matrix==class_id]=color\n",
    "\n",
    "        pred_image[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size] = predicted_patch_rgb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(pred_image.shape)\n",
    "\n",
    "\n",
    "bgr_pred = cv2.cvtColor(pred_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# cv2.imwrite(\"C:/Users/jiric/Documents/VUT/BP/DATASETS/CUSTOM_CZ_DATASET/Semantic segmentation dataset/pred_512_10_korDS_CE.png\", bgr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75be459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(np.unique(image_RGB))\n",
    "# pred_image = pred_image/255\n",
    "# print(np.unique(pred_image))\n",
    "# print(np.unique(bgr_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3e01a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nplt.figure(figsize=(15, 5))\\n\\nplt.subplot(1,2,1)\\nplt.imshow(image_RGB)\\nplt.title('Cropped image')\\nplt.axis('off')\\n\\nplt.subplot(1,2,2)\\nplt.imshow(pred_image)\\nplt.title('Cropped prediction')\\nplt.axis('off')\\n\\nplt.show()\\n\\n# print(np.unique(image_RGB))\\n# print(np.unique(pred_image))\\n\\nprint(path_model)\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(image_RGB)\n",
    "plt.title('Cropped image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(pred_image)\n",
    "plt.title('Cropped prediction')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# print(np.unique(image_RGB))\n",
    "# print(np.unique(pred_image))\n",
    "\n",
    "print(path_model)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50de2f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE SAVED\n"
     ]
    }
   ],
   "source": [
    "if save_bool:\n",
    "    cv2.imwrite(save_path, bgr_pred)\n",
    "    print(\"IMAGE SAVED\")\n",
    "else:\n",
    "    print(\"IMAGE NOT SAVED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf76c28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
